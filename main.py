# -*- coding: utf-8 -*-
"""ML_HW2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jH9NWBlppxPt8M8ddix3i27j-fIM8DHo
"""

# importing required modules
import numpy as np
import scipy.io as sio
import random
from sklearn.svm import SVC
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
from sklearn import model_selection
from sklearn.multiclass import OneVsRestClassifier
from numpy import linalg as LA
import time
from PIL import Image
import os, os.path
from zipfile import ZipFile 
import pandas as pd
from random import seed
from random import randint
import matplotlib.pyplot as plt

#/ specifying the zip file name 
zf = ZipFile('datasets.zip','r')
zf.extractall()
zf.close()

zf2 =  ZipFile('q1_dataset.zip', 'r')
zf2.extractall()
zf2.close() 

imgs = []
path = "van_gogh"
for f in os.listdir(path):
    im = np.array((Image.open(os.path.join(path,f))))
    if im.ndim == 2:
      width, height = im.shape
      out = np.empty((width, height, 3), dtype=np.uint8)
      out[:, :, 0] = im
      out[:, :, 1] = im
      out[:, :, 2] = im
      im = out 
    imgs.append(im)

X = []    
for image in imgs:
  pixels = image.flatten().reshape(4096, 3) 
  X.append(pixels)

Xr = np.array(X)[:,:,0]
Xg = np.array(X)[:,:,1]
Xb = np.array(X)[:,:,2]

# ___________________________________Q1.1_______________________________________

ur, sr, vhr = LA.svd(Xr, full_matrices=True);
ug, sg, vhg = LA.svd(Xg, full_matrices=True);
ub, sb, vhb = LA.svd(Xb, full_matrices=True);

x = np.arange(100)
fig = plt.figure()
ax1 = fig.add_axes([0,0,1,1])
ax1.bar(x,sr[0:100])
ax1.set_ylabel('Principal Components of Matrix Xi=R')
fig.show()

fig2 = plt.figure()
ax2 = fig2.add_axes([0,0,1,1])
ax2.bar(x,sg[0:100])
ax2.set_ylabel('Principal Components of Matrix Xi=G')
fig2.show()

fig3 = plt.figure()
ax3 = fig3.add_axes([0,0,1,1])
ax3.bar(x,sb[0:100])
ax3.set_ylabel('Principal Components of Matrix Xi=B')
fig3.show()


PVE_r = np.round(sr**2/np.sum(sr**2), decimals = 5)[0:10]
PVE_g = np.round(sg**2/np.sum(sg**2), decimals = 5)[0:10]
PVE_b = np.round(sb**2/np.sum(sb**2), decimals  =5)[0:10]

print('proportion of variance explained (PVE) for RED')
print(PVE_r)

print('proportion of variance explained (PVE) for Green')
print(PVE_g)

print('proportion of variance explained (PVE) for Blue')
print(PVE_b)

# ________________________________Q1.2__________________________________________
imgs_np = np.array(imgs)
mean_img = imgs_np.mean(axis = 0)
var_img = imgs_np.mean(axis = 0)
noise1 = np.random.normal(mean_img,np.sqrt(var_img), size = (64,64,3) ) * 0.01
noise_added = imgs_np[:] + noise1
XN = []    
for image in noise_added:
  pixels = image.flatten().reshape(4096, 3) 
  XN.append(pixels)

XNr = np.array(XN)[:,:,0]
XNg = np.array(XN)[:,:,1]
XNb = np.array(XN)[:,:,2]

uNr, sNr, vhNr = LA.svd(XNr, full_matrices=True);
uNg, sNg, vhNg = LA.svd(XNg, full_matrices=True);
uNb, sNb, vhNb = LA.svd(XNb, full_matrices=True);

x = np.arange(100)
fig = plt.figure()
ax1 = fig.add_axes([0,0,1,1])
ax1.bar(x,sNr[0:100])
ax1.set_ylabel('Principal Components of Noise added Xi:R')
fig.show()

fig2 = plt.figure()
ax2 = fig2.add_axes([0,0,1,1])
ax2.bar(x,sNg[0:100])
ax2.set_ylabel('Principal Components of Noise added Xi:G')
fig2.show()

fig3 = plt.figure()
ax3 = fig3.add_axes([0,0,1,1])
ax3.bar(x,sNb[0:100])
ax3.set_ylabel('Principal Components of Noise added Xi:B')
fig3.show()

# __________________________________Q2___________________________________
s = time.clock()
def training_lin_reg(fold,test):
  
  x1 = fold[:,[0,1,2,3,4,5,6]]
  y1 = fold[:,[7]]
  w = np.matmul(np.matmul(LA.inv(np.matmul(x1.T, x1)),x1.T),y1)
  #testing
  y_hat = np.matmul(np.array(test)[:,[0,1,2,3,4,5,6]],w)
  y2 = np.array(test)[:,[7]]
  y_bar = np.mean(y2, axis = 0)
  y_m_ybar = sum(np.square(y2 - y_bar))
  y_m_yhat = sum(np.square(y_hat - y_bar))
  R_sq = y_m_yhat/y_m_ybar
  MSE = np.mean(np.square(y2-y_hat))
  MAE = np.mean(abs(y_hat-y2))
  M = 0 
  for rV in range(len(y2)):
    M = M + (y2[rV] - y_hat[rV])*(1/y2[rV])
  MAPE = M/len(y2)

  return w,R_sq,MSE,MAE,MAPE
  
dataSet2 = pd.read_csv("q2_dataset.csv") 
shuffled = dataSet2.sample(frac=1)
Sp_data = np.array_split(shuffled, 5) 
fold1 = np.concatenate((Sp_data[0],Sp_data[1],Sp_data[2],Sp_data[3] ))
test1 = Sp_data[4]
fold2 = np.concatenate((Sp_data[0],Sp_data[1],Sp_data[2],Sp_data[4] ))
test2 = Sp_data[3]
fold3 = np.concatenate((Sp_data[0],Sp_data[1],Sp_data[3],Sp_data[4] ))
test3 = Sp_data[2]
fold4 = np.concatenate((Sp_data[0],Sp_data[2],Sp_data[3],Sp_data[4] ))
test4 = Sp_data[1]
fold5 = np.concatenate((Sp_data[1],Sp_data[2],Sp_data[3],Sp_data[4] ))
test5 = Sp_data[0]



w_fold1,R_sq1,MSE1,MAE1,MAPE1 = training_lin_reg(fold1,test1)
w_fold2,R_sq2,MSE2,MAE2,MAPE2 = training_lin_reg(fold2,test2)
w_fold3,R_sq3,MSE3,MAE3,MAPE3 = training_lin_reg(fold3,test3)
w_fold4,R_sq4,MSE4,MAE4,MAPE4 = training_lin_reg(fold4,test4)
w_fold5,R_sq5,MSE5,MAE5,MAPE5 = training_lin_reg(fold5,test5)

print('Fold1: R^2 =',R_sq1,', MSE =', MSE1,', MAE =',MAE1,', MAPE = ',MAPE1)
print('Fold2: R^2 =',R_sq2,', MSE =', MSE2,', MAE =',MAE2,', MAPE = ',MAPE2)
print('Fold3: R^2 =',R_sq3,', MSE =', MSE3,', MAE =',MAE3,', MAPE = ',MAPE3)
print('Fold4: R^2 =',R_sq4,', MSE =', MSE4,', MAE =',MAE4,', MAPE = ',MAPE4)
print('Fold5: R^2 =',R_sq5,', MSE =', MSE5,', MAE =',MAE5,', MAPE = ',MAPE5)

print("Run time for calculating linear Regression is",time.clock() - s, "seconds")

# __________________________________Q3___________________________________

# Part 3 Ridge Regression
def convert_d(data):
    cols = data.columns.values
    for column in cols:
        text = {}
        def convert(val):
            return text[val]
        if data[column].dtype != np.int32 and data[column].dtype != np.float32:
            contents = data[column].values.tolist()
            elements = set(contents)
            x = 0
            for unique in elements:
                if unique not in text:
                    text[unique] = x
                    x+=1
            data[column] = list(map(convert, data[column]))
    return data 

# Estimate logistic regression coefficients using gradient descent
def training_lgr(train,label, l_rate, iter, batch,key):
  coef = np.random.normal(scale = 0.01 , size = train.shape[1]+1).reshape(1,8)
  for it in range(iter):
    i = randint(0,train.shape[0] - batch )
    mini_batch = train[i:i + batch ,:]
    mini_label = label[i :i + batch]
    k= coef[:,1:]
    py = np.exp(coef[0,1] + np.sum(k*mini_batch, axis = 1))
    py1 = 1/py
    py0 = (py/(1+py)).reshape(1,batch)
    #gradient ascent algorithm
    error = mini_label - py0
		# sum_error += error**2
    for k in range(batch):  
      mP =  (l_rate*error[0,k]*py0[0,k]*(1.0 - py0[0,k]))
      coef[0,0] = coef[0,0] + mP
      for j in range(train.shape[1]):
        coef[0,j+1] = coef[0,j+1] + mP * train[k,j]
    if key == 1:
      print(coef)

  return coef    
# Printing Calculations
def calc_Param(weights, test_batch, testLabels):
  p_L = (weights[0,0] + np.sum(weights[0,1:]*test_batch,axis=1))>0
  False_Positive =  np.sum(testLabels[p_L[:]==1]==0)
  True_Positive =  np.sum(testLabels[p_L[:]==0]==0)
  False_Negative =  np.sum(testLabels[p_L[:]==0]==1)
  True_Negative =  np.sum(testLabels[p_L[:]==1]==1)
  matrix = np.empty((2,2))
  matrix[0,0] = True_Positive
  matrix[0,1] = False_Positive
  matrix[1,0] = False_Negative
  matrix[1,1] = True_Negative

  Precision = True_Positive/(True_Positive+False_Positive)
  Recall  = True_Positive/(True_Positive+False_Negative)
  F_measure1= 2*Precision*Recall/(Precision+Recall)
  F_measure2= 5*Precision*Recall/(4*Precision+Recall)
  NPV = True_Negative/(True_Negative+False_Negative)
  FPR =  False_Positive/(False_Positive+True_Positive)
  FDR = False_Positive/(True_Negative+False_Positive)
  acc = (np.sum(p_L[:] == testLabels[:])/len(testLabels))*100
  Class_Acc_P = True_Positive/(True_Positive + False_Positive)*100
  Class_Acc_N = True_Negative/(True_Negative + False_Negative)*100

  print('Precision = ', Precision,',Recall = ',Recall)
  print('F_measure1 = ', F_measure1,', F_measure2 = ',F_measure2)
  print('NPV = ', NPV,', FPR = ',FPR,', FDR = ',FDR )
  print('Class_Accuracy(pos) =', Class_Acc_P)
  print('Class_Accuracy(neg) =', Class_Acc_N)
  print('Accuracy =', acc)
  print('Confusion Matrix:')
  print(" ")
  print(matrix)

# ________________________________Q3____________________________________________
Data3_tr = pd.read_csv('q3_train_dataset.csv',sep=',')  
Data3_ts = pd.read_csv('q3_test_dataset.csv',sep=',')
Data3_tr = Data3_tr.sample(frac=1)
Data3_ts = Data3_ts.sample(frac=1)

df = np.array(convert_d(Data3_tr))
label = df[:,0]
df = df[:,1:]
testdF = np.array(convert_d(Data3_ts))
label_test = testdF[:,0]
testdF = testdF[:,1:]
iterations = 1000
l_rate = np.array([10**(-4),10**(-3),10**(-2)])

batch_size = 32
iterations = 1000
l_rate = np.array([10**(-4),10**(-3),10**(-2)])
start_time = time.clock()
AccuMini= []
Wmini = []
# MINI_BATCH GRADIENT_ASCENT
for rate in l_rate:
  x = training_lgr(df,label,rate,iterations,batch_size,0)
  Wmini.append(x.reshape(1,8))
  p_L1 = (x[0,0] + np.sum(x[0,1:]*testdF,axis=1))>0
  acc = (np.sum(p_L1[:] == label_test[:])/len(label_test))*100
  AccuMini.append(acc)

print("Total TrainTime for Q3.1 Mini_Batch is ",time.clock() - start_time, "seconds")
boo = np.argmax(AccuMini)
print(" ")

print("Mini_Batch Training Output with learning rate = ", l_rate[boo])
para = calc_Param(np.array(Wmini)[boo],testdF,label_test)

# Stochastic_Batch Gradient_Ascent
start_time = time.clock()
AccuSt = []
WSt = []
for rate in l_rate:
  x = training_lgr(df,label,rate,iterations,1,0)
  WSt.append(x.reshape(1,8))
  p_L = (x[0,0] + np.sum(x[0,1:]*testdF,axis=1))>0
  acc = (np.sum(p_L[:] == label_test[:])/len(label_test))*100
  AccuSt.append(acc)

print("Total TrainTime for Q3.1 Stochastic Batch Gradient is ",time.clock() - start_time, "seconds")

print(" " )
boo = np.argmax(AccuSt)
print("Stochastic_Batch Training Output with learning rate = ", l_rate[boo])
para = calc_Param(np.array(WSt)[boo],testdF,label_test)

# __________________________________Q3.3________________________________________
# full-bat Gradient
start_time = time.clock()
x = training_lgr(df,label,l_rate[boo],iterations,df.shape[0],1)
print("Total TrainTime for Q3.2 Full Batch Gradient is ",time.clock() - start_time, "seconds")
p_L = (x[0,0] + np.sum(x[0,1:]*testdF,axis=1))>0
print(" ")
print("full-bat Gradient with learning rate = ", 0.0001)
para = calc_Param(x,testdF,label_test)

# PART4 SYM# __________________________________Q4_______________________________
start_time = time.clock();
data = sio.loadmat('q4_dataset.mat', squeeze_me=False, chars_as_strings=False, mat_dtype=True, struct_as_record=True)
X = data['inception_features']
images = data['images']
y = data['class_labels'].ravel()


idx = np.random.permutation(len(X))
xS,yS = X[idx], y[idx]

fold = []
y_tr = []
xval = []
yval = []


Sp_data = np.array_split(xS, 5) 
Sp_label = np.array_split(yS,5)

fold.append(np.concatenate((Sp_data[2],Sp_data[3],Sp_data[4])))
y_tr.append(np.concatenate((Sp_label[2],Sp_label[3],Sp_label[4])))
xval.append(Sp_data[1])
yval.append(Sp_label[1])
fold.append( np.concatenate((Sp_data[0],Sp_data[3],Sp_data[4])))
y_tr.append(np.concatenate((Sp_label[0],Sp_label[3],Sp_label[4])))
xval.append(Sp_data[2])
yval.append(Sp_label[2])
fold.append(np.concatenate((Sp_data[0],Sp_data[1],Sp_data[4])))
y_tr.append(np.concatenate((Sp_label[0],Sp_label[1],Sp_label[4])))
xval.append(Sp_data[3])
yval.append(Sp_label[3])
fold.append(np.concatenate((Sp_data[0],Sp_data[1],Sp_data[2])))
y_tr.append(np.concatenate((Sp_label[0],Sp_label[1],Sp_label[2])))
xval.append(Sp_data[4])
yval.append(Sp_label[4])
fold.append( np.concatenate((Sp_data[1],Sp_data[2],Sp_data[3])))
y_tr.append(np.concatenate((Sp_label[1],Sp_label[2],Sp_label[3])))
xval.append( Sp_data[0])
yval.append(Sp_label[0])

t2 = time.clock()

arrayC = np.array([10**(-6), 10**(-4), 10**(-2), 1, 10, 10**10])
c_para_Acc = []
for i in range(len(arrayC)):
  acc = []
  for t in range(5):
    X_train = fold[t]
    y_train = y_tr[t]
    x_val = xval[t]
    y_val = yval[t]
    svclassifier = SVC(kernel='linear', C = arrayC[i])
    svclassifier.fit(X_train, y_train)
    y_pred_val = svclassifier.predict(x_val)
    # print(classification_report(y_test, y_pred))
    acc.append(accuracy_score(y_val, y_pred_val))
  c_para_Acc.append(np.mean(acc))
  print('Accuracy for C =',arrayC[i],'is',np.mean(acc),";Test on Validation Set")
Cs = arrayC[np.argmax(c_para_Acc)]
print('C hyper-parameter is ', Cs)
t1 = time.clock()
print("Run time for calculating best hyper parameter of SVM is",t1 - t2, "seconds")
foo = 0
skf = StratifiedKFold(n_splits=5, shuffle = True)
acc_linSVM_fold=[]
for train_index, test_index in skf.split(X, y):
  acc_fold = []
  X_train, X_test = X[train_index], X[test_index]
  y_train, y_test = y[train_index], y[test_index]
  svclassifier = SVC(kernel='linear', C = Cs)
  svclassifier.fit(X_train, y_train)
  y_pred = svclassifier.predict(X_test)
  acc = accuracy_score(y_test, y_pred)
  acc_linSVM_fold.append(acc)
  print('Accuracy for Cs =',Cs,'for fold',foo,'is',acc)
  foo = foo + 1
print("Run time for calculating Linear SVM is",time.clock() - t1, "seconds")
print("Total Runtime for Q4.1 is ",time.clock() - start_time, "seconds")

# non-linear SVM with radial basis function (RBF)
start_time = time.clock();
t1 = time.clock()
arrayC = np.array([10**(-4), 10**(-2), 1, 10, 10**10])
gm = [2**(-4),2**(-2),1,2**2,2**10,'scale']
acc_meter = np.zeros((len(gm),len(arrayC)))
for j in range(len(gm)):
  for i in range(len(arrayC)):
    acc = []
    for t in range(5):
      X_train = fold[t]
      y_train = y_tr[t]
      x_val = xval[t]
      y_val = yval[t]
      svclassifier = SVC(kernel='rbf', gamma = gm[j], C = arrayC[i])
      svclassifier.fit(X_train, y_train)
      y_pred_val = svclassifier.predict(x_val)
      # print(classification_report(y_test, y_pred))
      acc.append(accuracy_score(y_val, y_pred_val))
    acc_meter[j,i]=(np.mean(acc))
    # print('Accuracy for C =',arrayC[i],'is',np.mean(acc),";Test on Validation Set")
MaxCord = np.unravel_index(np.argmax(acc_meter), acc_meter.shape)
gamma_s = gm[MaxCord[0]]
Cs2 = arrayC[MaxCord[1]]
print('C hyper-parameter for rbf is ', Cs2)
print('gamma hyper-parameter for rbf is ', gamma_s)

print("Run time for best hyper parameters of non_linear SVM is",time.clock() - start_time, "seconds")
t1 = time.clock()

skf = StratifiedKFold(n_splits=5, shuffle = True)
foo = 0
acc_NonlinSVM_fold=[]
for train_index, test_index in skf.split(X, y):
  acc_fold = []
  X_train, X_test = X[train_index], X[test_index]
  y_train, y_test = y[train_index], y[test_index]
  svclassifier = SVC(kernel='rbf',gamma = gamma_s, C = Cs2)
  svclassifier.fit(X_train, y_train)
  y_pred = svclassifier.predict(X_test)
  acc = accuracy_score(y_test, y_pred)
  acc_NonlinSVM_fold.append(acc)
  print('Accuracy for Cs =',Cs,' and gamma =', gamma_s,'for fold',foo,'is',acc)
  foo = foo + 1
print("Run time for calculating Non-Linear SVM is",time.clock() - t1, "seconds")
print("Total Runtime for Q4.2 is ",time.clock() - start_time, "seconds")

# Q4.3
data = [acc_linSVM_fold, acc_NonlinSVM_fold] 
  
fig = plt.figure(figsize =(10, 7)) 
  
# Creating axes instance 
ax = fig.add_axes([0, 0, 1, 1]) 
ax.set_title('Comparing Performance Metrics')
ax.set_xlabel('Linear(1)and Non-linear SVM(2)')
ax.set_ylabel('Accuracy')
# Creating plot 
bp = ax.boxplot(data) 
  
# show plot 
plt.show()